from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class FrenchLLM:
    def __init__(self):
        # Mod√®le fran√ßais optimis√©
        self.model_name = "microsoft/DialoGPT-medium"  # Ou un mod√®le fran√ßais sp√©cifique
        self.tokenizer = None
        self.model = None
        
    def load_model(self):
        """Charge un mod√®le optimis√© pour le fran√ßais"""
        print("üì• Chargement du mod√®le fran√ßais...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Ajouter un token de padding si n√©cessaire
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("‚úÖ Mod√®le fran√ßais charg√© !")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur : {e}")
            return False
    
    def chat(self, message, max_length=150):
        """Chat en fran√ßais"""
        if not self.model or not self.tokenizer:
            return "Mod√®le non charg√©"
        
        # Encoder le message
        inputs = self.tokenizer.encode(message + self.tokenizer.eos_token, return_tensors="pt")
        
        # G√©n√©rer la r√©ponse
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # D√©coder la r√©ponse
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Retourner seulement la nouvelle partie
        return response[len(message):].strip()

# Test
if __name__ == "__main__":
    llm = FrenchLLM()
    if llm.load_model():
        response = llm.chat("Bonjour, pouvez-vous m'aider ?")
        print(f"R√©ponse : {response}")
