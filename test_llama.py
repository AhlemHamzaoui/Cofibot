import requests
import json
from datetime import datetime

def test_ollama_connection():
    """Test de connexion avec Ollama"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            print("âœ… Ollama est connectÃ© !")
            print("ğŸ“‹ ModÃ¨les disponibles:")
            for model in models:
                print(f"  - {model['name']} ({model['size']} bytes)")
            return True
        else:
            print("âŒ ProblÃ¨me de connexion avec Ollama")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama n'est pas en marche. Lance 'ollama serve' dans un terminal.")
        return False
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return False

def chat_with_llama(message, model="llama3.2:3b"):
    """Chat simple avec Llama"""
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": model,
        "prompt": message,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9,
            "max_tokens": 300
        }
    }
    
    try:
        print("ğŸ¤– Llama rÃ©flÃ©chit...")
        response = requests.post(url, json=payload)
        
        if response.status_code == 200:
            return response.json()["response"]
        else:
            return f"âŒ Erreur HTTP: {response.status_code}"
            
    except Exception as e:
        return f"âŒ Erreur: {str(e)}"

def chat_streaming(message, model="llama3.2:3b"):
    """Chat avec rÃ©ponse en temps rÃ©el"""
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": model,
        "prompt": message,
        "stream": True
    }
    
    try:
        response = requests.post(url, json=payload, stream=True)
        
        print("ğŸ¤– Llama: ", end="", flush=True)
        full_response = ""
        
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if "response" in data:
                    chunk = data["response"]
                    print(chunk, end="", flush=True)
                    full_response += chunk
                if data.get("done", False):
                    print("\n")
                    break
        
        return full_response
                    
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return ""

# Tests
if __name__ == "__main__":
    print("ğŸ§ª Test de Llama 3.2 3B")
    print("=" * 50)
    
    # Test de connexion
    if not test_ollama_connection():
        exit()
    
    print("\nğŸ¯ Test 1: Question simple")
    response = chat_with_llama("Bonjour ! Peux-tu te prÃ©senter en franÃ§ais ?")
    print(f"RÃ©ponse: {response}")
    
    print("\nğŸ¯ Test 2: Question sur Coficab")
    response = chat_with_llama("Que sais-tu sur l'industrie automobile et les cÃ¢bles Ã©lectriques ?")
    print(f"RÃ©ponse: {response}")
    
    print("\nğŸ¯ Test 3: Streaming (temps rÃ©el)")
    chat_streaming("Explique-moi en 3 points ce qu'est un chatbot.")
    
    print("\nâœ… Tests terminÃ©s !")
