from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

class LocalLLM:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        """
        Mod√®les recommand√©s :
        - microsoft/DialoGPT-medium (anglais, l√©ger)
        - microsoft/DialoGPT-large (anglais, plus lourd)
        - bigscience/bloom-560m (multilingue, l√©ger)
        - bigscience/bloom-1b7 (multilingue, moyen)
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
    def load_model(self):
        """Charge le mod√®le depuis Hugging Face"""
        print(f"üì• T√©l√©chargement du mod√®le {self.model_name}...")
        
        try:
            # Charger le tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Charger le mod√®le
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            # Cr√©er le pipeline
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            
            print("‚úÖ Mod√®le charg√© avec succ√®s !")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement : {e}")
            return False
    
    def generate_response(self, prompt, max_length=100):
        """G√©n√®re une r√©ponse √† partir d'un prompt"""
        if not self.pipeline:
            return "Mod√®le non charg√©"
        
        try:
            # G√©n√©rer la r√©ponse
            response = self.pipeline(
                prompt,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # Extraire le texte g√©n√©r√©
            generated_text = response[0]["generated_text"]
            
            # Retourner seulement la partie g√©n√©r√©e (sans le prompt)
            return generated_text[len(prompt):].strip()
            
        except Exception as e:
            return f"Erreur lors de la g√©n√©ration : {e}"

# Exemple d'utilisation
if __name__ == "__main__":
    # Initialiser le LLM
    llm = LocalLLM("microsoft/DialoGPT-medium")
    
    # Charger le mod√®le
    if llm.load_model():
        # Tester
        response = llm.generate_response("Bonjour, comment allez-vous ?")
        print(f"R√©ponse : {response}")
